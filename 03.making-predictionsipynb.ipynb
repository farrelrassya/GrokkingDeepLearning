{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farrelrassya/GrokkingDeepLearning/blob/main/03.making-predictionsipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sdTGSwZeGT1"
      },
      "source": [
        "# Chapter 3: Forward Propagation - Making a Prediction\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upxmvd3NeGT7"
      },
      "source": [
        "## Step 1: Predict\n",
        "\n",
        "This chapter is about prediction. In the previous chapter, you learned about the paradigm **predict, compare, learn**. In this chapter, we'll dive deep into the first step: predict.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/farrelrassya/GrokkingDeepLearning/main/folder-picture/03.chapter%203/picture%203.1.png\" width=\"700\">\n",
        "\n",
        "\n",
        "In this chapter, you'll learn more about what these three different parts of a neural network prediction look like under the hood. Let's start with the first one: the data.\n",
        "\n",
        "In your first neural network, you're going to predict one datapoint at a time, like so:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/farrelrassya/GrokkingDeepLearning/main/folder-picture/03.chapter%203/picture%203.2.png\" width=\"700\">\n",
        "\n",
        "\n",
        "Later, you'll find that the number of datapoints you process at a time has a significant impact on what a network looks like.\n",
        "\n",
        "You might be wondering, \"How do I choose how many datapoints to propagate at a time?\" The answer is based on whether you think the neural network can be accurate with the data you give it. For example, if I'm trying to predict whether there's a cat in a photo, I definitely need to show my network all the pixels of an image at once. Why? Well, if I sent you only one pixel of an image, could you classify whether the image contained a cat? Me neither!\n",
        "\n",
        "That's a general rule of thumb, by the way: always present enough information to the network, where \"enough information\" is defined loosely as how much a human might need to make the same prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82y9XB8CeGT9"
      },
      "source": [
        "---\n",
        "## Building Your First Neural Network\n",
        "\n",
        "Let's skip over the network for now. As it turns out, you can create a network only after you understand the shape of the input and output datasets (for now, shape means \"number of columns\" or \"number of datapoints you're processing at once\"). Let's stick with a single prediction of the likelihood that the baseball team will win:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/farrelrassya/GrokkingDeepLearning/main/folder-picture/03.chapter%203/picture%203.3.png\" width=\"700\">\n",
        "\n",
        "Now that you know you want to take one input datapoint and output one prediction, you can create a neural network. Because you have only one input datapoint and one output datapoint, you're going to build a network with a single knob mapping from the input point to the output. (Abstractly, these \"knobs\" are actually called **weights**, and I'll refer to them as such from here on out.)\n",
        "\n",
        "So, without further ado, here's your first neural network, with a single weight mapping from the input \"# toes\" to the output \"win?\":\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/farrelrassya/GrokkingDeepLearning/main/folder-picture/03.chapter%203/picture%203.4.png\" width=\"700\">\n",
        "\n",
        "As you can see, with one weight, this network takes in one datapoint at a time (average number of toes per player on the baseball team) and outputs a single prediction (whether it thinks the team will win)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFLCl9ZieGT_"
      },
      "source": [
        "---\n",
        "## A Simple Neural Network Making a Prediction\n",
        "\n",
        "Let's start with the simplest neural network possible.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/farrelrassya/GrokkingDeepLearning/main/folder-picture/03.chapter%203/picture%203.5.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrTiTVGLeGUA"
      },
      "source": [
        "---\n",
        "## What is a Neural Network?\n",
        "\n",
        "Here is your first neural network. To start a neural network, open a Jupyter notebook and run this code:\n",
        "\n",
        "```python\n",
        "# The network:\n",
        "\n",
        "weight = 0.1\n",
        "def neural_network(input, weight):\n",
        "    prediction = input * weight\n",
        "    return prediction\n",
        "\n",
        "# How we use the network to predict something:\n",
        "\n",
        "number_of_toes = [8.5, 9.5, 10, 9]\n",
        "input = number_of_toes[0]\n",
        "pred = neural_network(input, weight)\n",
        "print(pred)\n",
        "```\n",
        "\n",
        "You just made your first neural network and used it to predict! The last line prints the prediction (pred). It should be **0.85**.\n",
        "\n",
        "**What is a neural network?** For now, it's one or more weights that you can multiply by the input data to make a prediction.\n",
        "\n",
        "**What is input data?** It's a number that you recorded in the real world somewhere. It's usually something that is easily knowable, like today's temperature, a baseball player's batting average, or yesterday's stock price.\n",
        "\n",
        "**What is a prediction?** A prediction is what the neural network tells you, given the input data, such as \"given the temperature, it is 0% likely that people will wear sweatsuits today\" or \"given a baseball player's batting average, he is 30% likely to hit a home run\" or \"given yesterday's stock price, today's stock price will be 101.52.\"\n",
        "\n",
        "**Is this prediction always right?** No. Sometimes a neural network will make mistakes, but it can learn from them. For example, if it predicts too high, it will adjust its weight to predict lower next time, and vice versa.\n",
        "\n",
        "**How does the network learn?** Trial and error! First, it tries to make a prediction. Then, it sees whether the prediction was too high or too low. Finally, it changes the weight (up or down) to predict more accurately the next time it sees the same input."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The network:\n",
        "\n",
        "weight = 0.1\n",
        "def neural_network(input, weight):\n",
        "    prediction = input * weight\n",
        "    return prediction\n",
        "\n",
        "# How we use the network to predict something:\n",
        "\n",
        "number_of_toes = [8.5, 9.5, 10, 9]\n",
        "input = number_of_toes[0]\n",
        "pred = neural_network(input,weight)\n",
        "print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EVtJFWkOQph",
        "outputId": "79571438-a6f9-4778-e02f-a9cc91c5a412"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8500000000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SuQnfSSeGUB"
      },
      "source": [
        "---\n",
        "## What Does This Neural Network Do?\n",
        "\n",
        "**It multiplies the input by a weight. It \"scales\" the input by a certain amount.**\n",
        "\n",
        "In the previous section, you made your first prediction with a neural network. A neural network, in its simplest form, uses the power of multiplication. It takes an input datapoint (in this case, 8.5) and multiplies it by the weight.\n",
        "\n",
        "- If the weight is 2, then the neural network will double the input.\n",
        "- If the weight is 0.01, then the network will divide the input by 100.\n",
        "\n",
        "As you can see, some weight values make the input bigger, and other values make it smaller.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/farrelrassya/GrokkingDeepLearning/main/folder-picture/03.chapter%203/picture%203.6.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZ3R2hkJeGUC"
      },
      "source": [
        "---\n",
        "## The Neural Network Interface\n",
        "\n",
        "The interface for a neural network is simple. It accepts an **input** variable as information and a **weight** variable as knowledge and outputs a **prediction**. Every neural network you'll ever see works this way. It uses the knowledge in the weights to interpret the information in the input data. Later neural networks will accept larger, more complicated input and weight values, but this same underlying premise will always ring true.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/farrelrassya/GrokkingDeepLearning/main/folder-picture/03.chapter%203/picture%203.7.png\" width=\"700\">\n",
        "\n",
        "In this case, the information is the average number of toes on a baseball team before a game. Notice several things:\n",
        "\n",
        "First, the neural network does not have access to any information except one instance. If, after this prediction, you were to feed in `number_of_toes[1]`, the network wouldn't remember the prediction it made in the last timestep. A neural network knows only what you feed it as input. It forgets everything else. Later, you'll learn how to give a neural network a \"short-term memory\" by feeding in multiple inputs at once.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/farrelrassya/GrokkingDeepLearning/main/folder-picture/03.chapter%203/picture%203.8.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcRZXGIOeGUE"
      },
      "source": [
        "---\n",
        "## Weight as Sensitivity (Volume Knob)\n",
        "\n",
        "Another way to think about a neural network's weight value is as a measure of sensitivity between the input of the network and its prediction.\n",
        "\n",
        "- If the weight is very high, then even the tiniest input can create a really large prediction!\n",
        "- If the weight is very small, then even large inputs will make small predictions.\n",
        "\n",
        "This sensitivity is akin to volume. \"Turning up the weight\" amplifies the prediction relative to the input: **weight is a volume knob!**\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/farrelrassya/GrokkingDeepLearning/main/folder-picture/03.chapter%203/picture%203.9.png\" width=\"700\">\n",
        "\n",
        "In this case, what the neural network is really doing is applying a volume knob to the `number_of_toes` variable. In theory, this volume knob can tell you the likelihood that the team will win, based on the average number of toes per player on the team.\n",
        "\n",
        "This may or may not work. Truthfully, if the team members had an average of 0 toes, they would probably play terribly. But baseball is much more complex than this. In the next section, you'll present multiple pieces of information at the same time so the neural network can make more-informed decisions.\n",
        "\n",
        "Note that neural networks don't predict just positive numbers—they can also predict negative numbers and even take negative numbers as input. Perhaps you want to predict the probability that people will wear coats today. If the temperature is -10 degrees Celsius, then a negative weight will predict a high probability that people will wear their coats.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89lIif_MeGUF"
      },
      "source": [
        "---\n",
        "## Making a Prediction with Multiple Inputs\n",
        "\n",
        "**Neural networks can combine intelligence from multiple datapoints.**\n",
        "\n",
        "The previous neural network was able to take one datapoint as input and make one prediction based on that datapoint. Perhaps you've been wondering, \"Is the average number of toes really a good predictor, all by itself?\" If so, you're onto something.\n",
        "\n",
        "What if you could give the network more information (at one time) than just the average number of toes per player? In that case, the network should, in theory, be able to make more-accurate predictions. Well, as it turns out, a network can accept multiple input datapoints at a time.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/farrelrassya/GrokkingDeepLearning/main/folder-picture/03.chapter%203/picture%203.10.png\" width=\"700\">\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/farrelrassya/GrokkingDeepLearning/main/folder-picture/03.chapter%203/picture%203.11.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def w_sum(a,b):\n",
        "    assert(len(a) == len(b))\n",
        "    output = 0\n",
        "    for i in range(len(a)):\n",
        "        output += (a[i] * b[i])\n",
        "    return output\n",
        "\n",
        "weights = [0.1, 0.2, 0]\n",
        "\n",
        "def neural_network(input, weights):\n",
        "    pred = w_sum(input,weights)\n",
        "    return pred\n",
        "\n",
        "# This dataset is the current\n",
        "# status at the beginning of\n",
        "# each game for the first 4 games\n",
        "# in a season.\n",
        "\n",
        "# toes = current number of toes\n",
        "# wlrec = current games won (percent)\n",
        "# nfans = fan count (in millions)\n",
        "\n",
        "toes =  [8.5, 9.5, 9.9, 9.0]\n",
        "wlrec = [0.65, 0.8, 0.8, 0.9]\n",
        "nfans = [1.2, 1.3, 0.5, 1.0]\n",
        "\n",
        "# Input corresponds to every entry\n",
        "# for the first game of the season.\n",
        "\n",
        "input = [toes[0],wlrec[0],nfans[0]]\n",
        "pred = neural_network(input,weights)\n",
        "\n",
        "print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWf_niSTOXwe",
        "outputId": "ecc20755-e368-48df-ed65-d733438dcc84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9800000000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "weights = np.array([0.1, 0.2, 0])\n",
        "def neural_network(input, weights):\n",
        "    pred = input.dot(weights)\n",
        "    return pred\n",
        "\n",
        "toes =  np.array([8.5, 9.5, 9.9, 9.0])\n",
        "wlrec = np.array([0.65, 0.8, 0.8, 0.9])\n",
        "nfans = np.array([1.2, 1.3, 0.5, 1.0])\n",
        "\n",
        "# Input corresponds to every entry\n",
        "# for the first game of the season.\n",
        "\n",
        "input = np.array([toes[0],wlrec[0],nfans[0]])\n",
        "pred = neural_network(input,weights)\n",
        "\n",
        "print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDkVgid1OdP9",
        "outputId": "0319f72b-cc00-42a3-bf2e-6b0dd17f3a24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9800000000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0lIGihXeGUH"
      },
      "source": [
        "---\n",
        "## Multiple Inputs: What Does This Neural Network Do?\n",
        "\n",
        "**It multiplies three inputs by three knob weights and sums them. This is a weighted sum.**\n",
        "\n",
        "At the end of the previous section, you came to realize the limiting factor of your simple neural network: it was only a volume knob on one datapoint. In the example, that datapoint was a baseball team's average number of toes per player. You learned that in order to make accurate predictions, you need to build neural networks that can combine multiple inputs at the same time. Fortunately, neural networks are perfectly capable of doing so.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/farrelrassya/GrokkingDeepLearning/main/folder-picture/03.chapter%203/picture%203.12.png\" width=\"700\">\n",
        "\n",
        "This new neural network can accept multiple inputs at a time per prediction. This allows the network to combine various forms of information to make better-informed decisions. But the fundamental mechanism for using weights hasn't changed. You still take each input and run it through its own volume knob. In other words, you multiply each input by its own weight.\n",
        "\n",
        "The new property here is that, because you have multiple inputs, you have to sum their respective predictions. Thus, you multiply each input by its respective weight and then sum all the local predictions together. This is called a **weighted sum of the input**, or a **weighted sum** for short. Some also refer to the weighted sum as a **dot product**, as you'll see.\n",
        "\n",
        "**A relevant reminder:**\n",
        "\n",
        "The interface for the neural network is simple: it accepts an **input** variable as information and a **weights** variable as knowledge, and it outputs a **prediction**.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/farrelrassya/GrokkingDeepLearning/main/folder-picture/03.chapter%203/picture%203.13.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfygv3_ZeGUH"
      },
      "source": [
        "---\n",
        "## Vectors and Elementwise Operations\n",
        "\n",
        "This new need to process multiple inputs at a time justifies the use of a new tool. It's called a **vector**, and if you've been following along in your Jupyter notebook, you've already been using it. A vector is nothing other than a list of numbers. In the example, `input` is a vector and `weights` is a vector.\n",
        "\n",
        "Vectors are incredibly useful whenever you want to perform operations involving groups of numbers. In this case, you're performing a weighted sum between two vectors (a dot product). You're taking two vectors of equal length (`input` and `weights`), multiplying each number based on its position (the first position in `input` is multiplied by the first position in `weights`, and so on), and then summing the resulting output.\n",
        "\n",
        "Anytime you perform a mathematical operation between two vectors of equal length where you pair up values according to their position in the vector (position 0 with 0, 1 with 1, and so on), it's called an **elementwise operation**. Thus:\n",
        "\n",
        "- **Elementwise addition** sums two vectors\n",
        "- **Elementwise multiplication** multiplies two vectors\n",
        "\n",
        "**Challenge: Vector math**\n",
        "\n",
        "Being able to manipulate vectors is a cornerstone technique for deep learning. See if you can write functions that perform the following operations:\n",
        "\n",
        "- `def elementwise_multiplication(vec_a, vec_b)`\n",
        "- `def elementwise_addition(vec_a, vec_b)`\n",
        "- `def vector_sum(vec_a)`\n",
        "- `def vector_average(vec_a)`\n",
        "\n",
        "Then, see if you can use two of these methods to perform a dot product!\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/farrelrassya/GrokkingDeepLearning/main/folder-picture/03.chapter%203/picture%203.14.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9zZhvfQeGUI"
      },
      "source": [
        "---\n",
        "## Performing a Weighted Sum of Inputs\n",
        "\n",
        "```python\n",
        "def w_sum(a, b):\n",
        "    assert(len(a) == len(b))\n",
        "    output = 0\n",
        "    for i in range(len(a)):\n",
        "        output += (a[i] * b[i])\n",
        "    return output\n",
        "```\n",
        "\n",
        "**Example calculation:**\n",
        "\n",
        "| Inputs | Weights | Local Predictions |\n",
        "|--------|---------|-------------------|\n",
        "| 8.5 (toes) | 0.1 | (8.50 * 0.1) = 0.85 |\n",
        "| 0.65 (wlrec) | 0.2 | (0.65 * 0.2) = 0.13 |\n",
        "| 1.2 (fans) | 0.0 | (1.20 * 0.0) = 0.00 |\n",
        "\n",
        "**Final prediction:** toes prediction + wlrec prediction + fans prediction = 0.85 + 0.13 + 0.00 = **0.98**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xINd6xmVeGUI"
      },
      "source": [
        "---\n",
        "## The Intuition Behind the Dot Product\n",
        "\n",
        "The intuition behind how and why a dot product (weighted sum) works is easily one of the most important parts of truly understanding how neural networks make predictions. Loosely stated, a dot product gives you a notion of **similarity** between two vectors.\n",
        "\n",
        "Consider these examples:\n",
        "\n",
        "```\n",
        "a = [ 0, 1, 0, 1]\n",
        "b = [ 1, 0, 1, 0]\n",
        "c = [ 0, 1, 1, 0]\n",
        "d = [.5, 0,.5, 0]\n",
        "e = [ 0, 1,-1, 0]\n",
        "\n",
        "w_sum(a,b) = 0\n",
        "w_sum(b,c) = 1\n",
        "w_sum(b,d) = 1\n",
        "w_sum(c,c) = 2\n",
        "w_sum(d,d) = .5\n",
        "w_sum(c,e) = 0\n",
        "```\n",
        "\n",
        "The highest weighted sum (`w_sum(c,c)`) is between vectors that are exactly identical. In contrast, because `a` and `b` have no overlapping weight, their dot product is zero.\n",
        "\n",
        "Perhaps the most interesting weighted sum is between `c` and `e`, because `e` has a negative weight. This negative weight canceled out the positive similarity between them. But a dot product between `e` and itself would yield the number 2, despite the negative weight (double negative turns positive)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eE3IAAyeGUJ"
      },
      "source": [
        "---\n",
        "## Dot Product as Logical AND\n",
        "\n",
        "Sometimes you can equate the properties of the dot product to a **logical AND**.\n",
        "\n",
        "Consider `a` and `b`:\n",
        "```\n",
        "a = [ 0, 1, 0, 1]\n",
        "b = [ 1, 0, 1, 0]\n",
        "```\n",
        "\n",
        "If you ask whether both `a[0]` AND `b[0]` have value, the answer is no. If you ask whether both `a[1]` AND `b[1]` have value, the answer is again no. Because this is always true for all four values, the final score equals 0. Each value fails the logical AND.\n",
        "\n",
        "```\n",
        "b = [ 1, 0, 1, 0]\n",
        "c = [ 0, 1, 1, 0]\n",
        "```\n",
        "\n",
        "`b` and `c`, however, have one column that shares value. It passes the logical AND because `b[2]` and `c[2]` have weight. This column (and only this column) causes the score to rise to 1.\n",
        "\n",
        "```\n",
        "c = [ 0, 1, 1, 0]\n",
        "d = [.5, 0,.5, 0]\n",
        "```\n",
        "\n",
        "Fortunately, neural networks are also able to model **partial ANDing**. In this case, `c` and `d` share the same column as `b` and `c`, but because `d` has only 0.5 weight there, the final score is only 0.5. We exploit this property when modeling probabilities in neural networks.\n",
        "\n",
        "```\n",
        "d = [.5, 0,.5, 0]\n",
        "e = [-1, 1, 0, 0]\n",
        "```\n",
        "\n",
        "In this analogy, **negative weights** tend to imply a **logical NOT** operator, given that any positive weight paired with a negative weight will cause the score to go down. Furthermore, if both vectors have negative weights (such as `w_sum(e,e)`), then the neural network will perform a double negative and add weight instead.\n",
        "\n",
        "Additionally, some might say it's an OR after the AND, because if any of the rows show weight, the score is affected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iatGYG0eGUK"
      },
      "source": [
        "---\n",
        "## A Crude Language for Reading Weights\n",
        "\n",
        "This gives us a kind of crude language for reading weights. Let's read a few examples. These assume you're performing `w_sum(input, weights)` and the \"then\" to these if statements is an abstract \"then give high score\":\n",
        "\n",
        "```\n",
        "weights = [ 1, 0, 1]   => if input[0] OR input[2]\n",
        "weights = [ 0, 0, 1]   => if input[2]\n",
        "weights = [ 1, 0, -1]  => if input[0] OR NOT input[2]\n",
        "weights = [-1, 0, -1]  => if NOT input[0] OR NOT input[2]\n",
        "weights = [0.5, 0, 1]  => if BIG input[0] or input[2]\n",
        "```\n",
        "\n",
        "Notice in the last row that `weight[0] = 0.5` means the corresponding `input[0]` would have to be larger to compensate for the smaller weighting. This is a very crude approximate language, but it is immensely useful when trying to picture in your head what's going on under the hood. This will help you significantly in the future, especially when putting networks together in increasingly complex ways.\n",
        "\n",
        "Given these intuitions, what does this mean when a neural network makes a prediction? Roughly speaking, it means the network gives a high score of the inputs based on how similar they are to the weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQLGX2QYeGUK"
      },
      "source": [
        "---\n",
        "## Example: Predicting with Multiple Inputs\n",
        "\n",
        "Notice in the following example that `nfans` is completely ignored in the prediction because the weight associated with it is 0. The most sensitive predictor is `wlrec` because its weight is 0.2. But the dominant force in the high score is the number of toes (`ntoes`), not because the weight is the highest, but because the input combined with the weight is by far the highest.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/farrelrassya/GrokkingDeepLearning/main/folder-picture/03.chapter%203/picture%203.16.png\" width=\"700\">\n",
        "\n",
        "```python\n",
        "toes = [8.5, 9.5, 9.9, 9.0]\n",
        "wlrec = [0.65, 0.8, 0.8, 0.9]\n",
        "nfans = [1.2, 1.3, 0.5, 1.0]\n",
        "\n",
        "input = [toes[0], wlrec[0], nfans[0]]\n",
        "pred = neural_network(input, weights)\n",
        "print(pred)  # Output: 0.98\n",
        "```\n",
        "\n",
        "Input corresponds to every entry for the first game of the season.\n",
        "\n",
        "**Key points:**\n",
        "- You can't shuffle weights: they have specific positions they need to be in.\n",
        "- Both the value of the weight and the value of the input determine the overall impact on the final score.\n",
        "- A negative weight will cause some inputs to reduce the final prediction (and vice versa)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkGfdTfoeGUL"
      },
      "source": [
        "---\n",
        "## Complete Code: Basic Python vs NumPy\n",
        "\n",
        "**Previous code (basic Python):**\n",
        "\n",
        "```python\n",
        "def w_sum(a, b):\n",
        "    assert(len(a) == len(b))\n",
        "    output = 0\n",
        "    for i in range(len(a)):\n",
        "        output += (a[i] * b[i])\n",
        "    return output\n",
        "\n",
        "weights = [0.1, 0.2, 0]\n",
        "\n",
        "def neural_network(input, weights):\n",
        "    pred = w_sum(input, weights)\n",
        "    return pred\n",
        "\n",
        "toes = [8.5, 9.5, 9.9, 9.0]\n",
        "wlrec = [0.65, 0.8, 0.8, 0.9]\n",
        "nfans = [1.2, 1.3, 0.5, 1.0]\n",
        "\n",
        "input = [toes[0], wlrec[0], nfans[0]]\n",
        "pred = neural_network(input, weights)\n",
        "print(pred)\n",
        "```\n",
        "\n",
        "**NumPy code:**\n",
        "\n",
        "There's a Python library called NumPy, which stands for \"numerical Python.\" It has very efficient code for creating vectors and performing common functions (such as dot products).\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "weights = np.array([0.1, 0.2, 0])\n",
        "\n",
        "def neural_network(input, weights):\n",
        "    pred = input.dot(weights)\n",
        "    return pred\n",
        "\n",
        "toes = np.array([8.5, 9.5, 9.9, 9.0])\n",
        "wlrec = np.array([0.65, 0.8, 0.8, 0.9])\n",
        "nfans = np.array([1.2, 1.3, 0.5, 1.0])\n",
        "\n",
        "input = np.array([toes[0], wlrec[0], nfans[0]])\n",
        "pred = neural_network(input, weights)\n",
        "print(pred)\n",
        "```\n",
        "\n",
        "Both networks should print out **0.98**. Notice that in the NumPy code, you don't have to create a `w_sum` function. Instead, NumPy has a `dot` function (short for \"dot product\") you can call. Many functions you'll use in the future have NumPy parallels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1k81LDIeGUL"
      },
      "source": [
        "---\n",
        "## Making a Prediction with Multiple Outputs\n",
        "\n",
        "**Neural networks can also make multiple predictions using only a single input.**\n",
        "\n",
        "Perhaps a simpler augmentation than multiple inputs is multiple outputs. Prediction occurs the same as if there were three disconnected single-weight neural networks.\n",
        "\n",
        "The most important comment in this setting is to notice that the three predictions are completely separate. Unlike neural networks with multiple inputs and a single output, where the prediction is undeniably connected, this network truly behaves as three independent components, each receiving the same input data. This makes the network simple to implement.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/farrelrassya/GrokkingDeepLearning/main/folder-picture/03.chapter%203/picture%203.17.png\" width=\"700\">\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/farrelrassya/GrokkingDeepLearning/main/folder-picture/03.chapter%203/picture%203.18.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instead of predicting just\n",
        "# whether the team won or lost,\n",
        "# now we're also predicting whether\n",
        "# they are happy/sad AND the percentage\n",
        "# of the team that is hurt. We are\n",
        "# making this prediction using only\n",
        "# the current win/loss record.\n",
        "\n",
        "def ele_mul(number,vector):\n",
        "    output = [0,0,0]\n",
        "    assert(len(output) == len(vector))\n",
        "    for i in range(len(vector)):\n",
        "        output[i] = number * vector[i]\n",
        "    return output\n",
        "\n",
        "weights = [0.3, 0.2, 0.9]\n",
        "\n",
        "def neural_network(input, weights):\n",
        "    pred = ele_mul(input,weights)\n",
        "    return pred\n",
        "\n",
        "wlrec = [0.65, 0.8, 0.8, 0.9]\n",
        "input = wlrec[0]\n",
        "pred = neural_network(input,weights)\n",
        "\n",
        "print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67sULdxtOkwf",
        "outputId": "a493d569-8ace-4eaf-ee7a-793ccebc84b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.195, 0.13, 0.5850000000000001]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/farrelrassya/GrokkingDeepLearning/main/folder-picture/03.chapter%203/picture%203.19.png\" width=\"700\">\n",
        "\n"
      ],
      "metadata": {
        "id": "NhDEQwN3NTZI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9YZ7zYzeGUL"
      },
      "source": [
        "---\n",
        "## Predicting with Multiple Inputs and Outputs\n",
        "\n",
        "**Neural networks can predict multiple outputs given multiple inputs.**\n",
        "\n",
        "Finally, the way you build a network with multiple inputs or outputs can be combined to build a network that has both multiple inputs and multiple outputs. As before, a weight connects each input node to each output node, and prediction occurs in the usual way."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/farrelrassya/GrokkingDeepLearning/main/folder-picture/03.chapter%203/picture%203.20.png\" width=\"700\">"
      ],
      "metadata": {
        "id": "H7EBt2NlNbfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/farrelrassya/GrokkingDeepLearning/main/folder-picture/03.chapter%203/picture%203.21.png\" width=\"700\">"
      ],
      "metadata": {
        "id": "L5f443JmNd0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "            #toes %win #fans\n",
        "weights = [ [0.1, 0.1, -0.3], #hurt?\n",
        "            [0.1, 0.2, 0.0], #win?\n",
        "            [0.0, 1.3, 0.1] ] #sad?\n",
        "\n",
        "def w_sum(a,b):\n",
        "    assert(len(a) == len(b))\n",
        "    output = 0\n",
        "    for i in range(len(a)):\n",
        "        output += (a[i] * b[i])\n",
        "    return output\n",
        "\n",
        "def vect_mat_mul(vect,matrix):\n",
        "    assert(len(vect) == len(matrix))\n",
        "    output = [0,0,0]\n",
        "    for i in range(len(vect)):\n",
        "        output[i] = w_sum(vect,matrix[i])\n",
        "    return output\n",
        "\n",
        "def neural_network(input, weights):\n",
        "    pred = vect_mat_mul(input,weights)\n",
        "    return pred\n",
        "\n",
        "# This dataset is the current\n",
        "# status at the beginning of\n",
        "# each game for the first 4 games\n",
        "# in a season.\n",
        "\n",
        "# toes = current number of toes\n",
        "# wlrec = current games won (percent)\n",
        "# nfans = fan count (in millions)\n",
        "\n",
        "toes =  [8.5, 9.5, 9.9, 9.0]\n",
        "wlrec = [0.65,0.8, 0.8, 0.9]\n",
        "nfans = [1.2, 1.3, 0.5, 1.0]\n",
        "\n",
        "# Input corresponds to every entry\n",
        "# for the first game of the season.\n",
        "\n",
        "input = [toes[0],wlrec[0],nfans[0]]\n",
        "pred = neural_network(input,weights)\n",
        "\n",
        "print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9m2K3VwOnWC",
        "outputId": "dab35020-72fe-4b36-8e45-0401de5415f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.555, 0.9800000000000001, 0.9650000000000001]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VknVtH07eGUL"
      },
      "source": [
        "---\n",
        "## Multiple Inputs and Outputs: How Does It Work?\n",
        "\n",
        "**It performs three independent weighted sums of the input to make three predictions.**\n",
        "\n",
        "You can take two perspectives on this architecture:\n",
        "1. Think of it as three weights coming out of each input node, or\n",
        "2. Three weights going into each output node\n",
        "\n",
        "For now, the latter is much more beneficial. Think about this neural network as three independent dot products: three independent weighted sums of the input. Each output node takes its own weighted sum of the input and makes a prediction.\n",
        "\n",
        "We're choosing to think about this network as a series of weighted sums. The code creates a new function called `vect_mat_mul`. This function iterates through each row of weights (each row is a vector) and makes a prediction using the `w_sum` function. It's literally performing three consecutive weighted sums and then storing their predictions in a vector called `output`.\n",
        "\n",
        "A lot more weights are flying around in this one, but it isn't that much more advanced than other networks you've seen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UpbEevKeGUM"
      },
      "source": [
        "---\n",
        "## Matrices and Vector-Matrix Multiplication\n",
        "\n",
        "The `weights` variable is a list of vectors. A list of vectors is called a **matrix**. It's as simple as it sounds.\n",
        "\n",
        "Commonly used functions use matrices. One of these is called **vector-matrix multiplication**. The series of weighted sums is exactly that: you take a vector and perform a dot product with every row in a matrix.\n",
        "\n",
        "NumPy has special functions to help with this operation."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Predicting on predictions, Neural networks can be stacked!\n",
        "\n",
        "As the following figures make clear, you can also take the output of one network and feed it as input to another network. This results in two consecutive `vector-matrix multiplications`. It may not yet be clear why you’d predict this way; but some datasets (such as image classification) contain patterns that are too complex for a single-weight matrix. Later, we’ll\n",
        "discuss the nature of these patterns. For now, it’s sufficient to know this is possible.\n",
        "\n",
        "The following listing shows how you can do the same operations coded in the previous\n",
        "section using a convenient Python library called NumPy. Using libraries like NumPy makes\n",
        "your code faster and easier to read and write."
      ],
      "metadata": {
        "id": "_hBqrqhCp946"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/farrelrassya/GrokkingDeepLearning/main/folder-picture/03.chapter%203/picture%203.24.png\" width=\"700\">\n",
        "\n"
      ],
      "metadata": {
        "id": "tB7EvwArNr1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#toes %win #fans\n",
        "ih_wgt = np.array([\n",
        "            [0.1, 0.2, -0.1], #hid[0]\n",
        "            [-0.1,0.1, 0.9], #hid[1]\n",
        "            [0.1, 0.4, 0.1]]).T #hid[2]\n",
        "\n",
        "\n",
        "# hid[0] hid[1] hid[2]\n",
        "hp_wgt = np.array([\n",
        "            [0.3, 1.1, -0.3], #hurt?\n",
        "            [0.1, 0.2, 0.0], #win?\n",
        "            [0.0, 1.3, 0.1] ]).T #sad?\n",
        "\n",
        "weights = [ih_wgt, hp_wgt]\n",
        "\n",
        "def neural_network(input, weights):\n",
        "\n",
        "    hid = input.dot(weights[0])\n",
        "    pred = hid.dot(weights[1])\n",
        "    return pred\n",
        "\n",
        "\n",
        "toes =  np.array([8.5, 9.5, 9.9, 9.0])\n",
        "wlrec = np.array([0.65,0.8, 0.8, 0.9])\n",
        "nfans = np.array([1.2, 1.3, 0.5, 1.0])\n",
        "\n",
        "input = np.array([toes[0],wlrec[0],nfans[0]])\n",
        "\n",
        "pred = neural_network(input,weights)\n",
        "print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rg2vb3OPp9Rv",
        "outputId": "e19da036-2b4e-4089-cf10-34dddcdecd58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.2135 0.145  0.5065]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src=\"https://raw.githubusercontent.com/farrelrassya/GrokkingDeepLearning/main/folder-picture/03.chapter%203/picture%203.25.png\" width=\"700\">"
      ],
      "metadata": {
        "id": "SxUb9-CwOIa4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A quick primer on NumPy\n",
        "NumPy does a few things for you. Let’s reveal the magic.\n",
        "So far in this chapter, we’ve discussed two new types of mathematical tools: vectors and matrices. You’ve also learned about different operations that occur on vectors and matrices, including dot\n",
        "products, elementwise multiplication and addition, and vector-matrix multiplication. For these operations, you’ve written Python functions that can operate on simple Python list objects.\n",
        "\n",
        "In the short term, you’ll keep writing and using these functions to be sure you fully understand what’s going on inside them. But now that I’ve mentioned NumPy and several of the big operations, I’d like to give you a quick rundown of basic NumPy use so you’ll be ready for the transition toNumPy-only chapters. Let’s start with the basics again: vectors and matrices.\n",
        "\n",
        "You can create vectors and matrices in multiple ways in NumPy. Most of the common techniques for neural networks are listed in the previous code. Note that the processes for creating a vector and a matrix are identical. If you create a matrix with only one row, you’re creating a vector. And, as\n",
        "in mathematics in general, you create a matrix by listing (rows,columns).\n",
        "I say that only so you can remember the order: rows come first, columns\n",
        "come second. Let’s see some operations you can perform on these vectors and\n",
        "matrices:"
      ],
      "metadata": {
        "id": "dJlBk_mOy7tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.array([0,1,2,3]) # a vector\n",
        "b = np.array([4,5,6,7]) # another vector\n",
        "c = np.array([[0,1,2,3], # a matrix\n",
        "              [4,5,6,7]])"
      ],
      "metadata": {
        "id": "fUB9yCwYwc2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = np.zeros((2,4)) # (2x4 matrix of zeros)\n",
        "e = np.random.rand(2,5) # random 2x5\n",
        "# matrix with all numbers between 0 and 1"
      ],
      "metadata": {
        "id": "bUvmf3C8zYvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(a)\n",
        "print(b)\n",
        "print(c)\n",
        "print(d)\n",
        "print(e)"
      ],
      "metadata": {
        "id": "tmNGtFjozaf4",
        "outputId": "bba78000-0c8e-40ed-b737-eb3d76c1f771",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 2 3]\n",
            "[4 5 6 7]\n",
            "[[0 1 2 3]\n",
            " [4 5 6 7]]\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "[[0.89098929 0.0927021  0.84967105 0.77384292 0.37071797]\n",
            " [0.72632208 0.78448528 0.28300585 0.59051581 0.39563813]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# NumPy's Automatic Operation Detection\n",
        "\n",
        "Go ahead and run all of the previous code. The first bit of \"at first confusing but eventually heavenly\" magic should be visible. When you multiply two variables with the `*` function, NumPy automatically detects what kinds of variables you're working with and tries to figure out the operation you're talking about.\n",
        "\n",
        "This can be mega-convenient but sometimes makes NumPy code a bit hard to read. Make sure you keep track of each variable type as you go along.\n",
        "\n",
        "**The general rule of thumb for anything elementwise (+, –, *, /):**\n",
        "\n",
        "Either the two variables must have the same number of columns, or one of the variables must have only one column.\n",
        "\n",
        "---\n",
        "\n",
        "## Examples of NumPy's Automatic Detection\n",
        "\n",
        "**Vector-scalar multiplication:**\n",
        "\n",
        "`print(a * 0.1)` multiplies a vector by a single number (a scalar). NumPy says, \"Oh, I bet I'm supposed to do vector-scalar multiplication here,\" and then multiplies the scalar (0.1) by every value in the vector.\n",
        "\n",
        "**Scalar-matrix multiplication:**\n",
        "\n",
        "`print(c * 0.2)` looks exactly the same, except NumPy knows that `c` is a matrix. Thus, it performs scalar-matrix multiplication, multiplying every element in `c` by 0.2. Because the scalar has only one column, you can multiply it by anything (or divide, add, or subtract).\n",
        "\n",
        "**Elementwise vector multiplication:**\n",
        "\n",
        "`print(a * b)` — NumPy first identifies that they're both vectors. Because neither vector has only one column, NumPy checks whether they have an identical number of columns. They do, so NumPy knows to multiply each element by each element, based on their positions in the vectors. The same is true with addition, subtraction, and division.\n",
        "\n",
        "**Vector-matrix multiplication:**\n",
        "\n",
        "`print(a * c)` is perhaps the most elusive. `a` is a vector with four columns, and `c` is a (2 × 4) matrix. Neither has only one column, so NumPy checks whether they have the same number of columns. They do, so NumPy multiplies the vector `a` by each row of `c` (as if it were doing elementwise vector multiplication on each row).\n"
      ],
      "metadata": {
        "id": "PxdNRU4aKB8_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7laP7yFVN5k5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(a * 0.1) # multiplies every number in vector \"a\" by 0.1\n",
        "\n",
        "print(c * 0.2) # multiplies every number in matrix \"c\" by 0.2\n",
        "\n",
        "print(a * b) # multiplies elementwise between a and b (columns paired up)\n",
        "\n",
        "print(a * b * 0.2) # elementwise multiplication then multiplied by 0.2\n",
        "\n",
        "print(a * c) # since c has the same number of columns as a, this performs\n",
        "# elementwise multiplication on every row of the matrix \"c\""
      ],
      "metadata": {
        "id": "pXaVpCS0zb3X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a69093d1-faad-403e-df01-fa7ba4701b7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.  0.1 0.2 0.3]\n",
            "[[0.  0.2 0.4 0.6]\n",
            " [0.8 1.  1.2 1.4]]\n",
            "[ 0  5 12 21]\n",
            "[0.  1.  2.4 4.2]\n",
            "[[ 0  1  4  9]\n",
            " [ 0  5 12 21]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(a * e) # since a and e don't have the same number of columns, this\n",
        "# throws a \"Value Error: operands could not be broadcast together with..\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "aJoe3NgDKXYD",
        "outputId": "f3afaafb-3d45-4d07-8383-eb001d1e0260"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "operands could not be broadcast together with shapes (4,) (2,5) ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2510780080.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# since a and e don't have the same number of columns, this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# throws a \"Value Error: operands could not be broadcast together with..\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4,) (2,5) "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.zeros((1,4)) # vector of length 4\n",
        "b = np.zeros((4,3)) # matrix with 4 rows & 3 columns\n",
        "\n",
        "c = a.dot(b)\n",
        "print(c.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JcAqHxoKcVe",
        "outputId": "aee7029a-620b-412e-e1a4-f5bf2c94f983"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Reading NumPy Code\n",
        "\n",
        "The most confusing part is that all of these operations look the same if you don't know which variables are scalars, vectors, or matrices.\n",
        "\n",
        "When you \"read NumPy,\" you're really doing two things:\n",
        "1. Reading the operations\n",
        "2. Keeping track of the shape (number of rows and columns) of each operation\n",
        "\n",
        "It will take some practice, but eventually it becomes second nature.\n",
        "\n",
        "---\n",
        "\n",
        "## The Golden Rule of the Dot Function\n",
        "\n",
        "**If you put the (rows, cols) description of the two variables you're \"dotting\" next to each other, neighboring numbers should always be the same.**\n",
        "\n",
        "Example: You're dot-producting (1,4) with (4,3). It works fine and outputs (1,3).\n",
        "\n",
        "In terms of variable shape, regardless of whether you're dotting vectors or matrices: their shape (number of rows and columns) must line up. The columns of the left matrix must equal the rows on the right:\n",
        "\n",
        "```\n",
        "(a,b).dot(b,c) = (a,c)\n",
        "```"
      ],
      "metadata": {
        "id": "HDnUsf0sKS52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.zeros((2,4)) # matrix with 2 rows and 4 columns\n",
        "b = np.zeros((4,3)) # matrix with 4 rows & 3 columns\n",
        "\n",
        "c = a.dot(b)\n",
        "print(c.shape) # outputs (2,3)\n",
        "\n",
        "e = np.zeros((2,1)) # matrix with 2 rows and 1 columns\n",
        "f = np.zeros((1,3)) # matrix with 1 row & 3 columns\n",
        "\n",
        "g = e.dot(f)\n",
        "print(g.shape) # outputs (2,3)\n",
        "\n",
        "h = np.zeros((5,4)).T # matrix with 4 rows and 5 columns\n",
        "i = np.zeros((5,6)) # matrix with 6 rows & 5 columns\n",
        "\n",
        "j = h.dot(i)\n",
        "print(j.shape) # outputs (4,6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fBwvClXKSgV",
        "outputId": "552e08ed-e876-4466-b108-d4e25d00dc23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 3)\n",
            "(2, 3)\n",
            "(4, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "h = np.zeros((5,4)) # matrix with 5 rows and 4 columns\n",
        "i = np.zeros((5,6)) # matrix with 5 rows & 6 columns\n",
        "j = h.dot(i)\n",
        "print(j.shape) # throws an error"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "lIhx0iBRKg5q",
        "outputId": "8670183a-a046-4d12-bcbf-eedd545f713b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "shapes (5,4) and (5,6) not aligned: 4 (dim 1) != 5 (dim 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2661435304.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# matrix with 5 rows and 4 columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# matrix with 5 rows & 6 columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# throws an error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (5,4) and (5,6) not aligned: 4 (dim 1) != 5 (dim 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "To predict, neural networks perform repeated weighted sums of the input.\n",
        "\n",
        "You've seen an increasingly complex variety of neural networks in this chapter. A relatively small number of simple rules are used repeatedly to create larger, more advanced neural networks. The network's intelligence depends on the weight values you give it.\n",
        "\n",
        "Everything we've done in this chapter is a form of what's called **forward propagation**, wherein a neural network takes input data and makes a prediction. It's called this because you're propagating activations forward through the network.\n",
        "\n",
        "In these examples, **activations** are all the numbers that are not weights and are unique for every prediction.\n",
        "\n",
        "In the next chapter, you'll learn how to set weights so your neural networks make accurate predictions. Just as prediction is based on several simple techniques that are repeated/stacked on top of each other, weight learning is also a series of simple techniques that are combined many times across an architecture.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "d1tmQwnIKFXO"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}